{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741350a1",
   "metadata": {},
   "source": [
    "### Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b07110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    # CHANGE THIS TO THE PATH TO YOUR u.ITEM FILE:\n",
    "    with codecs.open(\"C:/spark_data/ml-100k/u.ITEM\", \"r\", encoding='ISO-8859-1', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ALSExample\").getOrCreate()\n",
    "    \n",
    "moviesSchema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "    \n",
    "names = loadMovieNames()\n",
    "    \n",
    "ratings = spark.read.option(\"sep\", \"\\t\").schema(moviesSchema) \\\n",
    "    .csv(\"C:/spark_data/ml-100k/u.data\")\n",
    "    \n",
    "print(\"Training recommendation model...\")\n",
    "\n",
    "als = ALS().setMaxIter(5).setRegParam(0.01).setUserCol(\"userID\").setItemCol(\"movieID\") \\\n",
    "    .setRatingCol(\"rating\")\n",
    "    \n",
    "model = als.fit(ratings)\n",
    "\n",
    "# Manually construct a dataframe of the user ID's we want recs for\n",
    "userID = int(sys.argv[1])\n",
    "userSchema = StructType([StructField(\"userID\", IntegerType(), True)])\n",
    "users = spark.createDataFrame([[userID,]], userSchema)\n",
    "\n",
    "recommendations = model.recommendForUserSubset(users, 10).collect()\n",
    "\n",
    "print(\"Top 10 recommendations for user ID \" + str(userID))\n",
    "\n",
    "for userRecs in recommendations:\n",
    "    myRecs = userRecs[1]  #userRecs is (userID, [Row(movieId, rating), Row(movieID, rating)...])\n",
    "    for rec in myRecs: #my Recs is just the column of recs for the user\n",
    "        movie = rec[0] #For each rec in the list, extract the movie ID and rating\n",
    "        rating = rec[1]\n",
    "        movieName = names[movie]\n",
    "        print(movieName + str(rating))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5545f8db",
   "metadata": {},
   "source": [
    "###  Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a SparkSession (Note, the config section is only for Windows!)\n",
    "    spark = SparkSession.builder.appName(\"LinearRegression\").getOrCreate()\n",
    "\n",
    "    # Load up our data and convert it to the format MLLib expects.\n",
    "    inputLines = spark.sparkContext.textFile(\"regression.txt\")\n",
    "    data = inputLines.map(lambda x: x.split(\",\")).map(lambda x: (float(x[0]), Vectors.dense(float(x[1]))))\n",
    "\n",
    "    # Convert this RDD to a DataFrame\n",
    "    colNames = [\"label\", \"features\"]\n",
    "    df = data.toDF(colNames)\n",
    "\n",
    "    # Note, there are lots of cases where you can avoid going from an RDD to a DataFrame.\n",
    "    # Perhaps you're importing data from a real database. Or you are using structured streaming\n",
    "    # to get your data.\n",
    "\n",
    "    # Let's split our data into training data and testing data\n",
    "    trainTest = df.randomSplit([0.5, 0.5])\n",
    "    trainingDF = trainTest[0]\n",
    "    testDF = trainTest[1]\n",
    "\n",
    "    # Now create our linear regression model\n",
    "    lir = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "    # Train the model using our training data\n",
    "    model = lir.fit(trainingDF)\n",
    "\n",
    "    # Now see if we can predict values in our test data.\n",
    "    # Generate predictions using our linear regression model for all features in our\n",
    "    # test dataframe:\n",
    "    fullPredictions = model.transform(testDF).cache()\n",
    "\n",
    "    # Extract the predictions and the \"known\" correct labels.\n",
    "    predictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "    labels = fullPredictions.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "    # Zip them together\n",
    "    predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "    # Print out the predicted and actual values for each point\n",
    "    for prediction in predictionAndLabel:\n",
    "      print(prediction)\n",
    "\n",
    "\n",
    "    # Stop the session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f9585",
   "metadata": {},
   "source": [
    "### Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037514c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a SparkSession (Note, the config section is only for Windows!)\n",
    "    spark = SparkSession.builder.appName(\"DecisionTree\").getOrCreate()\n",
    "\n",
    "    \n",
    "    # Load up data as dataframe\n",
    "    data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "        .csv(\"C:/spark_data/realestate.csv\")\n",
    "\n",
    "    assembler = VectorAssembler().setInputCols([\"HouseAge\", \"DistanceToMRT\", \\\n",
    "                               \"NumberConvenienceStores\"]).setOutputCol(\"features\")\n",
    "    \n",
    "    df = assembler.transform(data).select(\"PriceOfUnitArea\", \"features\")\n",
    "\n",
    "    # Let's split our data into training data and testing data\n",
    "    trainTest = df.randomSplit([0.5, 0.5])\n",
    "    trainingDF = trainTest[0]\n",
    "    testDF = trainTest[1]\n",
    "\n",
    "    # Now create our decision tree\n",
    "    dtr = DecisionTreeRegressor().setFeaturesCol(\"features\").setLabelCol(\"PriceOfUnitArea\")\n",
    "\n",
    "    # Train the model using our training data\n",
    "    model = dtr.fit(trainingDF)\n",
    "\n",
    "    # Now see if we can predict values in our test data.\n",
    "    # Generate predictions using our decision tree model for all features in our\n",
    "    # test dataframe:\n",
    "    fullPredictions = model.transform(testDF).cache()\n",
    "\n",
    "    # Extract the predictions and the \"known\" correct labels.\n",
    "    predictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "    labels = fullPredictions.select(\"PriceOfUnitArea\").rdd.map(lambda x: x[0])\n",
    "\n",
    "    # Zip them together\n",
    "    predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "    # Print out the predicted and actual values for each point\n",
    "    for prediction in predictionAndLabel:\n",
    "      print(prediction)\n",
    "\n",
    "\n",
    "    # Stop the session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ae50b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b57bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce88ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a03c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
