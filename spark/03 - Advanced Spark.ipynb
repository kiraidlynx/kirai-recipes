{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5356255",
   "metadata": {},
   "source": [
    "### Popular movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55166ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\n",
    "\n",
    "# Create schema when reading u.data\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "\n",
    "# Load up movie data as dataframe\n",
    "moviesDF = spark.read.option(\"sep\", \"\\t\").schema(schema).csv(\"C:/spark_data/ml-100k/u.data\")\n",
    "\n",
    "# Some SQL-style magic to sort all movies by popularity in one line!\n",
    "topMovieIDs = moviesDF.groupBy(\"movieID\").count().orderBy(func.desc(\"count\"))\n",
    "\n",
    "# Grab the top 10\n",
    "topMovieIDs.show(10)\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48144e6",
   "metadata": {},
   "source": [
    "###  Broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1709f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType\n",
    "import codecs\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    # CHANGE THIS TO THE PATH TO YOUR u.ITEM FILE:\n",
    "    with codecs.open(\"C:/spark_data/ml-100k/u.ITEM\", \"r\", encoding='ISO-8859-1', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\n",
    "\n",
    "nameDict = spark.sparkContext.broadcast(loadMovieNames())\n",
    "\n",
    "# Create schema when reading u.data\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "\n",
    "# Load up movie data as dataframe\n",
    "moviesDF = spark.read.option(\"sep\", \"\\t\").schema(schema).csv(\"C:/spark_data/ml-100k/u.data\")\n",
    "\n",
    "movieCounts = moviesDF.groupBy(\"movieID\").count()\n",
    "\n",
    "# Create a user-defined function to look up movie names from our broadcasted dictionary\n",
    "def lookupName(movieID):\n",
    "    return nameDict.value[movieID]\n",
    "\n",
    "lookupNameUDF = func.udf(lookupName)\n",
    "\n",
    "# Add a movieTitle column using our new udf\n",
    "moviesWithNames = movieCounts.withColumn(\"movieTitle\", lookupNameUDF(func.col(\"movieID\")))\n",
    "\n",
    "# Sort the results\n",
    "sortedMoviesWithNames = moviesWithNames.orderBy(func.desc(\"count\"))\n",
    "\n",
    "# Grab the top 10\n",
    "sortedMoviesWithNames.show(10, False)\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d164595e",
   "metadata": {},
   "source": [
    "### Super-hero graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MostPopularSuperhero\").getOrCreate()\n",
    "\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"id\", IntegerType(), True), \\\n",
    "                     StructField(\"name\", StringType(), True)])\n",
    "\n",
    "names = spark.read.schema(schema).option(\"sep\", \" \").csv(\"C:/spark_data/Marvel-names.txt\")\n",
    "\n",
    "lines = spark.read.text(\"C:/spark_data/Marvel-graph.txt\")\n",
    "\n",
    "# Small tweak vs. what's shown in the video: we trim each line of whitespace as that could\n",
    "# throw off the counts.\n",
    "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
    "    .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
    "    .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
    "    \n",
    "mostPopular = connections.sort(func.col(\"connections\").desc()).first()\n",
    "\n",
    "mostPopularName = names.filter(func.col(\"id\") == mostPopular[0]).select(\"name\").first()\n",
    "\n",
    "print(mostPopularName[0] + \" is the most popular superhero with \" + str(mostPopular[1]) + \" co-appearances.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31acbd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MostObscureSuperheroes\").getOrCreate()\n",
    "\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"id\", IntegerType(), True), \\\n",
    "                     StructField(\"name\", StringType(), True)])\n",
    "\n",
    "names = spark.read.schema(schema).option(\"sep\", \" \").csv(\"C:/spark_data/Marvel-names.txt\")\n",
    "\n",
    "lines = spark.read.text(\"C:/spark_data/Marvel-graph.txt\")\n",
    "\n",
    "# Small tweak vs. what's shown in the video: we trim whitespace from each line as this\n",
    "# could throw the counts off by one.\n",
    "connections = lines.withColumn(\"id\", func.split(func.trim(func.col(\"value\")), \" \")[0]) \\\n",
    "    .withColumn(\"connections\", func.size(func.split(func.trim(func.col(\"value\")), \" \")) - 1) \\\n",
    "    .groupBy(\"id\").agg(func.sum(\"connections\").alias(\"connections\"))\n",
    "    \n",
    "minConnectionCount = connections.agg(func.min(\"connections\")).first()[0]\n",
    "\n",
    "minConnections = connections.filter(func.col(\"connections\") == minConnectionCount)\n",
    "\n",
    "minConnectionsWithNames = minConnections.join(names, \"id\")\n",
    "\n",
    "print(\"The following characters have only \" + str(minConnectionCount) + \" connection(s):\")\n",
    "\n",
    "minConnectionsWithNames.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a121e77",
   "metadata": {},
   "source": [
    "### Degrees of separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42899aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boilerplate stuff:\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"DegreesOfSeparation\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# The characters we wish to find the degree of separation between:\n",
    "startCharacterID = 5306 #SpiderMan\n",
    "targetCharacterID = 14  #ADAM 3,031 (who?)\n",
    "\n",
    "# Our accumulator, used to signal when we find the target character during\n",
    "# our BFS traversal.\n",
    "hitCounter = sc.accumulator(0)\n",
    "\n",
    "def convertToBFS(line):\n",
    "    fields = line.split()\n",
    "    heroID = int(fields[0])\n",
    "    connections = []\n",
    "    for connection in fields[1:]:\n",
    "        connections.append(int(connection))\n",
    "\n",
    "    color = 'WHITE'\n",
    "    distance = 9999\n",
    "\n",
    "    if (heroID == startCharacterID):\n",
    "        color = 'GRAY'\n",
    "        distance = 0\n",
    "\n",
    "    return (heroID, (connections, distance, color))\n",
    "\n",
    "\n",
    "def createStartingRdd():\n",
    "    inputFile = sc.textFile(\"C:/spark_data/marvel-graph.txt\")\n",
    "    return inputFile.map(convertToBFS)\n",
    "\n",
    "def bfsMap(node):\n",
    "    characterID = node[0]\n",
    "    data = node[1]\n",
    "    connections = data[0]\n",
    "    distance = data[1]\n",
    "    color = data[2]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    #If this node needs to be expanded...\n",
    "    if (color == 'GRAY'):\n",
    "        for connection in connections:\n",
    "            newCharacterID = connection\n",
    "            newDistance = distance + 1\n",
    "            newColor = 'GRAY'\n",
    "            if (targetCharacterID == connection):\n",
    "                hitCounter.add(1)\n",
    "\n",
    "            newEntry = (newCharacterID, ([], newDistance, newColor))\n",
    "            results.append(newEntry)\n",
    "\n",
    "        #We've processed this node, so color it black\n",
    "        color = 'BLACK'\n",
    "\n",
    "    #Emit the input node so we don't lose it.\n",
    "    results.append( (characterID, (connections, distance, color)) )\n",
    "    return results\n",
    "\n",
    "def bfsReduce(data1, data2):\n",
    "    edges1 = data1[0]\n",
    "    edges2 = data2[0]\n",
    "    distance1 = data1[1]\n",
    "    distance2 = data2[1]\n",
    "    color1 = data1[2]\n",
    "    color2 = data2[2]\n",
    "\n",
    "    distance = 9999\n",
    "    color = color1\n",
    "    edges = []\n",
    "\n",
    "    # See if one is the original node with its connections.\n",
    "    # If so preserve them.\n",
    "    if (len(edges1) > 0):\n",
    "        edges.extend(edges1)\n",
    "    if (len(edges2) > 0):\n",
    "        edges.extend(edges2)\n",
    "\n",
    "    # Preserve minimum distance\n",
    "    if (distance1 < distance):\n",
    "        distance = distance1\n",
    "\n",
    "    if (distance2 < distance):\n",
    "        distance = distance2\n",
    "\n",
    "    # Preserve darkest color\n",
    "    if (color1 == 'WHITE' and (color2 == 'GRAY' or color2 == 'BLACK')):\n",
    "        color = color2\n",
    "\n",
    "    if (color1 == 'GRAY' and color2 == 'BLACK'):\n",
    "        color = color2\n",
    "\n",
    "    if (color2 == 'WHITE' and (color1 == 'GRAY' or color1 == 'BLACK')):\n",
    "        color = color1\n",
    "\n",
    "    if (color2 == 'GRAY' and color1 == 'BLACK'):\n",
    "        color = color1\n",
    "\n",
    "    return (edges, distance, color)\n",
    "\n",
    "\n",
    "#Main program here:\n",
    "iterationRdd = createStartingRdd()\n",
    "\n",
    "for iteration in range(0, 10):\n",
    "    print(\"Running BFS iteration# \" + str(iteration+1))\n",
    "\n",
    "    # Create new vertices as needed to darken or reduce distances in the\n",
    "    # reduce stage. If we encounter the node we're looking for as a GRAY\n",
    "    # node, increment our accumulator to signal that we're done.\n",
    "    mapped = iterationRdd.flatMap(bfsMap)\n",
    "\n",
    "    # Note that mapped.count() action here forces the RDD to be evaluated, and\n",
    "    # that's the only reason our accumulator is actually updated.\n",
    "    print(\"Processing \" + str(mapped.count()) + \" values.\")\n",
    "\n",
    "    if (hitCounter.value > 0):\n",
    "        print(\"Hit the target character! From \" + str(hitCounter.value) \\\n",
    "            + \" different direction(s).\")\n",
    "        break\n",
    "\n",
    "    # Reducer combines data for each character ID, preserving the darkest\n",
    "    # color and shortest path.\n",
    "    iterationRdd = mapped.reduceByKey(bfsReduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041c679",
   "metadata": {},
   "source": [
    "### Item-based collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c0b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "import sys\n",
    "\n",
    "def computeCosineSimilarity(spark, data):\n",
    "    # Compute xx, xy and yy columns\n",
    "    pairScores = data \\\n",
    "      .withColumn(\"xx\", func.col(\"rating1\") * func.col(\"rating1\")) \\\n",
    "      .withColumn(\"yy\", func.col(\"rating2\") * func.col(\"rating2\")) \\\n",
    "      .withColumn(\"xy\", func.col(\"rating1\") * func.col(\"rating2\")) \n",
    "\n",
    "    # Compute numerator, denominator and numPairs columns\n",
    "    calculateSimilarity = pairScores \\\n",
    "      .groupBy(\"movie1\", \"movie2\") \\\n",
    "      .agg( \\\n",
    "        func.sum(func.col(\"xy\")).alias(\"numerator\"), \\\n",
    "        (func.sqrt(func.sum(func.col(\"xx\"))) * func.sqrt(func.sum(func.col(\"yy\")))).alias(\"denominator\"), \\\n",
    "        func.count(func.col(\"xy\")).alias(\"numPairs\")\n",
    "      )\n",
    "\n",
    "    # Calculate score and select only needed columns (movie1, movie2, score, numPairs)\n",
    "    result = calculateSimilarity \\\n",
    "      .withColumn(\"score\", \\\n",
    "        func.when(func.col(\"denominator\") != 0, func.col(\"numerator\") / func.col(\"denominator\")) \\\n",
    "          .otherwise(0) \\\n",
    "      ).select(\"movie1\", \"movie2\", \"score\", \"numPairs\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Get movie name by given movie id \n",
    "def getMovieName(movieNames, movieId):\n",
    "    result = movieNames.filter(func.col(\"movieID\") == movieId) \\\n",
    "        .select(\"movieTitle\").collect()[0]\n",
    "\n",
    "    return result[0]\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MovieSimilarities\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "movieNamesSchema = StructType([ \\\n",
    "                               StructField(\"movieID\", IntegerType(), True), \\\n",
    "                               StructField(\"movieTitle\", StringType(), True) \\\n",
    "                               ])\n",
    "    \n",
    "moviesSchema = StructType([ \\\n",
    "                     StructField(\"userID\", IntegerType(), True), \\\n",
    "                     StructField(\"movieID\", IntegerType(), True), \\\n",
    "                     StructField(\"rating\", IntegerType(), True), \\\n",
    "                     StructField(\"timestamp\", LongType(), True)])\n",
    "    \n",
    "    \n",
    "# Create a broadcast dataset of movieID and movieTitle.\n",
    "# Apply ISO-885901 charset\n",
    "movieNames = spark.read \\\n",
    "      .option(\"sep\", \"|\") \\\n",
    "      .option(\"charset\", \"ISO-8859-1\") \\\n",
    "      .schema(movieNamesSchema) \\\n",
    "      .csv(\"file:///SparkCourse/ml-100k/u.item\")\n",
    "\n",
    "# Load up movie data as dataset\n",
    "movies = spark.read \\\n",
    "      .option(\"sep\", \"\\t\") \\\n",
    "      .schema(moviesSchema) \\\n",
    "      .csv(\"file:///SparkCourse/ml-100k/u.data\")\n",
    "\n",
    "\n",
    "ratings = movies.select(\"userId\", \"movieId\", \"rating\")\n",
    "\n",
    "# Emit every movie rated together by the same user.\n",
    "# Self-join to find every combination.\n",
    "# Select movie pairs and rating pairs\n",
    "moviePairs = ratings.alias(\"ratings1\") \\\n",
    "      .join(ratings.alias(\"ratings2\"), (func.col(\"ratings1.userId\") == func.col(\"ratings2.userId\")) \\\n",
    "            & (func.col(\"ratings1.movieId\") < func.col(\"ratings2.movieId\"))) \\\n",
    "      .select(func.col(\"ratings1.movieId\").alias(\"movie1\"), \\\n",
    "        func.col(\"ratings2.movieId\").alias(\"movie2\"), \\\n",
    "        func.col(\"ratings1.rating\").alias(\"rating1\"), \\\n",
    "        func.col(\"ratings2.rating\").alias(\"rating2\"))\n",
    "\n",
    "\n",
    "moviePairSimilarities = computeCosineSimilarity(spark, moviePairs).cache()\n",
    "\n",
    "if (len(sys.argv) > 1):\n",
    "    scoreThreshold = 0.97\n",
    "    coOccurrenceThreshold = 50.0\n",
    "\n",
    "    movieID = int(sys.argv[1])\n",
    "\n",
    "    # Filter for movies with this sim that are \"good\" as defined by\n",
    "    # our quality thresholds above\n",
    "    filteredResults = moviePairSimilarities.filter( \\\n",
    "        ((func.col(\"movie1\") == movieID) | (func.col(\"movie2\") == movieID)) & \\\n",
    "          (func.col(\"score\") > scoreThreshold) & (func.col(\"numPairs\") > coOccurrenceThreshold))\n",
    "\n",
    "    # Sort by quality score.\n",
    "    results = filteredResults.sort(func.col(\"score\").desc()).take(10)\n",
    "    \n",
    "    print (\"Top 10 similar movies for \" + getMovieName(movieNames, movieID))\n",
    "    \n",
    "    for result in results:\n",
    "        # Display the similarity result that isn't the movie we're looking at\n",
    "        similarMovieID = result.movie1\n",
    "        if (similarMovieID == movieID):\n",
    "          similarMovieID = result.movie2\n",
    "        \n",
    "        print(getMovieName(movieNames, similarMovieID) + \"\\tscore: \" \\\n",
    "              + str(result.score) + \"\\tstrength: \" + str(result.numPairs))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f48ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896c35b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
