---
title: "05 - regression"
author: "Ccanadas"
date: "2022-09-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Evaluar el error

rmsr<-sqrt(mean(x_obs-x_pred)^2)
plot(x_obs, x_pred)
abline(0,1)

## knn regresión

library(dummies)
library(FNN)
library(scales)
library(caret)

1º) categorías por factores a través de dummies (necesitamos variables numéricas)
2º)estandarizar predictores con rescale
3º) partición de datos


```{r part, eval=FALSE}
t.id <- createDataPartition(df$y_col, p=0.6, list = F)
tr <- df[t.id, ]
temp <- df[-t.id, ]
v.id <- createDataPartition(df$y_col, p=0.5, list = F)
val <- temp[v.id,]
test <- temp[-v.id,]
```



4º)construir modelos para distintos k y evaluar a través del rmse

```{r knn_manual, eval=FALSE}

reg <- knn.reg(tr[,cols_x], val[,cols_x], tr$col_y, k=n,
                algorithm = "brute")
rmse <- sqrt(mean((reg$pred-df$col_y)^2))

```



5º) plot (rmse para cada K) y aplicar el óptimo al conjunto testing

`rmse1 <- sqrt(mean((df$pred-df$col)^2))`

errors = c(rmse1, rmse2, rmse3, rmse4)

plot(errors, type = 'o', xlab = "k", ylab = "RMSE")




## Funciones para automatizar el knn

```{r knn_reg, eval=FALSE}

##Función para automatizar KNN
rda.knn.reg <- function(tr_predictor, val_predictors,
                          tr_target, val_target, k){
  library(FNN)
  res <- knn.reg(tr_predictor, val_predictors,
                 tr_target, k, algorithm = "brute")
  rmserror <- sqrt(mean((val_target - res$pred)^2))
  cat(paste("RMSE para k = ", toString(k), ": ", rmserror,"\n", sep = ""))
  rmserror
}

##Función para realizar múltiples KNN
rda.knn.reg.multi <- function(tr_predictors, val_predictors,
                                tr_target, val_target, start_k, end_k){
  rms_errors <- vector()
  for(k in start_k:end_k){
    rms_error <- rdacb.knn.reg(tr_predictors, val_predictors,
                               tr_target, val_target, k)
    rms_errors <- c(rms_errors, rms_error)
  }
  plot(rms_errors, type = 'o', xlab = "k", ylab = "RMSE")
}

```


## Regresión lineal

```{r lm, eval=FALSE}

mod<-lm(y~., data=df[tr.ids,])

#pintar el modelo

par((mfrow)=c(2,2))
plot(mod)

#Teoría de la representacion
```


Factor de referencia: df<-within(df, col<-relevel(col,ref="nivel)): utilizar la que tenga más elementos

Función STEP: libr(MASS); step.model<-stepAIC(mod, direction="backwards"): selecciona variables más importantes a partir de las CP

(REPASO)

# Métodos de regresión avanzados

## Árbol de regresión
library(rpart), library(rpart.plot)

```{r reg_tree, eval=FALSE}

#modelo
dfit<-rpart(y~., data=df[tr.ids,])

#plot
prp(dfit, type=2, nn=T, fallen.leaves=T, faclen=4, varlen=8)


```


información del modelo: dfit$cptable (x_error+sd<x_error(n))
plotcp(dfit): usar regla del codo


dfit.pruned<-prune(dfit, cp=CP(n))

PREDICCIÖN:

preds<-predict(dfit.pruned, df[-tr.ids,])
rsme(preds, -df[tr.ids])

### BAGGING 

Se usa para alta varianza, combina de forma conjunta las preds de diferentes modelos por separado para dar un resultado mejor.

library(ipred)

bagfit<-bagging(y~., dara=df)

### BOOSTING

reducir el desvío cuando los nuevos modelos se construyen para aprender de los errores de la mala clasificación de los modelos previos

library(gbm)

boostfit<-gbm(y~., data=df, distrib="gaussian")


## Regression RANDOM FOREST



```{r rf_reg, eval=FALSE}
mod <- randomForest(x = df[t.id, cols_x], y = df[t.id, col_y],
                    ntree = 1000, 
                    xtest = df[-t.id, cols_x], ytest = df[-t.id, col_y],
                    importance = T, keep.forest = T)

mod

mod$importance

#mtry = m/3, donde m = # de predictores
#nodesize = tamaño de nodos terminales para ser considerado
#maxnodes terminales
```


plot(df[tr.ids,], predict(mod, newdata=df[tr.ids,]), xlabel="actual",ylabel="pred")
plot(df[-tr.ids,], predict(mod, newdata=df[-tr.ids,]), xlabel="actual",ylabel="pred")




## Redes neuronales para regresión

library(nnet), library(devtools)

1º partición: encontrar el rando de variabildiad de la respuesta y reescalarlo

2º modelo

```{r nnet_reg, eval=FALSE}
fit<-nnet(y/max(y)~., data=df[tr.ids,], size=6, decay=.01, maxit=1000, linout=T)

```

3º) paquete git para la representación

```{r plotnnet, eval=FALSE}
source_url("https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r")

plot(fit, max.sp = T)




```


4º) evaluación del modelo:

sqrt(mean((fit$fitted.values*max(y)-df[t.id,"y_col"])^2))


pred <- predict(fit, df[-t.id,])
sqrt(mean((pred*max(y) -  df[-t.id,"y_col"])^2))


## Algoritmos de CROSS VALIDATION


CAMBIAR LA TARGET VAR: MEDV y poner en entrada

implementa la CROSS VAL. para regresión linea, rehacer para los demás modelos de regresión cambiando solo el modelo lm por el que toca.
 
```{r crossval, eval=FALSE}


kfold.crossval.reg <- function(df, nfolds){
  fold <- sample(1:nfolds, nrow(df), replace = T)
  cat(fold)
  
  mean.sqr.errs <- sapply(1:nfolds, 
                          kfold.cval.reg.iter,
                          df, fold)
  
  list("MSE "= mean.sqr.errs,
       "Overall_Mean_Sqr_Error" = mean(mean.sqr.errs),
       "Std_Mean_Sqr_Error" = sd(mean.sqr.errs))
}

kfold.cval.reg.iter <- function(k, df, fold){
  
  tr.ids <- !fold %in% c(k)
  test.ids <- fold %in% c(k)
  mod <- lm(MEDV ~., data = df[tr.ids,])
  pred <- predict(mod, df[test.ids,])
  sqr.errs <- (pred - df[test.ids,"MEDV"])^2
  mean(sqr.errs)
}




loocv.reg <- function(df){
  mean.sqr.errors <- sapply(1:nrow(df), 
                            loocv.reg.iter, df)
  list("MSE"=mean.sqr.errors,
       "overall_mean_sqr_errors" = mean(mean.sqr.errors),
       "sd_mean_sqr_errors" = sd(mean.sqr.errors))
}

loocv.reg.iter <- function(k, df){
  mod <- lm(MEDV~., data = df[-k,])
  pred <- predict(mod, df[k,])
  sqr.error <- (pred - df[k,"MEDV"])^2
  sqr.error
}





```

LOOCV - leave one out cross validation (más duro computacionalmente)
















